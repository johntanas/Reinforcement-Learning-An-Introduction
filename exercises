Exercise 1.1: Self-Play Suppose, instead of playing against a random
opponent, the reinforcement learning algorithm described above played against
itself. What do you think would happen in this case? Would it learn a different
way of playing?

The agent would be unable to learn anything optimal as both players are random.
It would learn a differentway of playing

Exercise 1.2: Symmetries Many tic-tac-toe positions appear different but
are really the same because of symmetries. How might we amend the reinforcement learning algorithm described above to take advantage of this? In what
ways would this improve it? Now think again. Suppose the opponent did not
take advantage of symmetries. In that case, should we? Is it true, then, that
symmetrically equivalent positions should necessarily have the same value?

We can check for game states and transpose the game state 3 times when checking each time.
We should still take adv of sym.
The game state would still be logically equivalent.

Exercise 1.3: Greedy Play Suppose the reinforcement learning player was
greedy, that is, it always played the move that brought it to the position that
it rated the best. Would it learn to play better, or worse, than a nongreedy
player? What problems might occur?

The player would player better at first before hitting a local maximum as it did not try seemingly -ev plays that might be actually +ev in the future.
Thus exploration moves are needed to get to global maximum

Exercise 1.4: Learning from Exploration Suppose learning updates occurred
after all moves, including exploratory moves. If the step-size parameter is
appropriately reduced over time, then the state values would converge to a
set of probabilities. What are the two sets of probabilities computed when we
do, and when we do not, learn from exploratory moves? Assuming that we
do continue to make exploratory moves, which set of probabilities might be
better to learn? Which would result in more wins?

do not learn -> local maximum
learn -> global maximum

Exercise 1.5: Other Improvements Can you think of other ways to improve
the reinforcement learning player? Can you think of any better way to solve
the tic-tac-toe problem as posed?

I think predefined heuristics in weights might be more efficient using prior knowledge of the game.

Exercise 2.1 In the comparison shown in Figure 2.1, which method will
perform best in the long run in terms of cumulative reward and cumulative
probability of selecting the best action? How much better will it be? Express
your answer quantitatively

e=0.1, 2x better

Exercise 2.2 Give pseudocode for a complete algorithm for the n-armed
bandit problem. Use greedy action selection and incremental computation of
action values with α = 1/k
step-size parameter. Assume a function bandit(a)
that takes an action and returns a reward. Use arrays and variables; do not
subscript anything by the time index t (for examples of this style of pseudocode, see Figures 4.1 and 4.3). Indicate how the action values are initialized
and updated after each reward. Indicate how the step-size parameters are set
for each action as a function of how many times it has been tried.

rewards=[0]*n
usedamounts=[0]*n
for i in range(t):
choice= max index of rewards
currentreward= bandit(choice)
alpha = 1/usedamounts[choice] if usedamounts[choice] != 0 else 1
rewards[choice]= rewards[choice]+alpha*currentreward

Exercise 2.3 If the step-size parameters, αk, are not constant, then the estimate Qk is a weighted average of previously received rewards with a weighting
different from that given by (2.6). What is the weighting on each prior reward
for the general case, analogous to (2.6), in terms of αk?

αk(1-αk)^(i-1)

Exercise 2.5 The results shown in Figure 2.2 should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of
the curve for the optimistic method? What might make this method perform
particularly better or worse, on average, on particular early plays?
E(action) =0 <<<5 and actios have a variance on expected reward.
if the less optimal actions chosen at first, the most optimal action will be chosen and the greatest ev and vice versa.

Exercise 2.6 Suppose you face a binary bandit task whose true action values
change randomly from play to play. Specifically, suppose that for any play the
true values of actions 1 and 2 are respectively 0.1 and 0.2 with probability 0.5
(case A), and 0.9 and 0.8 with probability 0.5 (case B). If you are not able to
tell which case you face at any play, what is the best expectation of success
you can achieve and how should you behave to achieve it? Now suppose that
on each play you are told if you are facing case A or case B (although you still
don’t know the true action values). This is an associative search task. What
is the best expectation of success you can achieve in this task, and how should
you behave to achieve it?

apply greedy approach


rewards[choice]= rewards[choice]*alpha

Exercise 4.1 If π is the equiprobable random policy, what is qπ(11, down)?
What is qπ(7, down)?
-1
-15
Exercise 4.2 Suppose a new state 15 is added to the gridworld just below
state 13, and its actions, left, up, right, and down, take the agent to states
12, 13, 14, and 15, respectively. Assume that the transitions from the original
states are unchanged. What, then, is vπ(15) for the equiprobable random
policy? Now suppose the dynamics of state 13 are also changed, such that
action down from state 13 takes the agent to the new state 15. What is vπ(15)
for the equiprobable random policy in this case?
